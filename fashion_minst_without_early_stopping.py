# -*- coding: utf-8 -*-
"""Fashion_MINST_Without Early Stopping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cLO1BK-iXkjX8Y6nIPFm4ggiXYfkFeVb
"""

import tensorflow as tf
print("TensorFlow version:", tf.__version__)

"""#### LOADING DATA & PREPROCESSING"""

# Step 1: LIBRARIES LOADING FUNCTION
import numpy as np
from numpy import mean, std
from matplotlib import pyplot as plt
import seaborn as sns
from keras.datasets import fashion_mnist

# Step 2: LOAD DATA FUNCTION-Loading the Raw Dataset
def load_data():
    return fashion_mnist.load_data()

# Step 3: SPLIT DATA FUNCTION-Dataset Splitting into Cross-Validation and Test Datasets
def split_data(X_test, y_test):
    # Split the X_test/y_test dataset into cross-val & test datasets
    # Randomly sort X_test/y_test
    indexes = np.arange(X_test.shape[0])
    for _ in range(5): indexes = np.random.permutation(indexes)  # shuffle 5 times!
    X_test = X_test[indexes]
    y_test = y_test[indexes]

    # Split into cross-val & test sets (use 8000 records in cross-val set;80%:20% Ratio)
    val_count = 8000
    X_val = X_test[:val_count]
    y_val = y_test[:val_count]
    X_test = X_test[val_count:]
    y_test = y_test[val_count:]

    # Keep a non-preprocessed copy of X_test/y_test for visualization
    test_images, test_labels = X_test.copy(), y_test.copy()

    return X_val, y_val, X_test, y_test, test_images, test_labels

# Step 4: SCALE IMAGES FUNCTION-Scaling the Images
def scale_images(X_train, X_val, X_test):
    # Scale the images to between 0-1
    X_train = X_train.astype('float32') / 255.0
    X_val = X_val.astype('float32') / 255.0
    X_test = X_test.astype('float32') / 255.0

    return X_train, X_val, X_test

# Step 5: RESHAPE IMAGES FUNCTION-Reshaping the Images into 3D Tensors
def reshape_images(X_train, X_val, X_test):
    # Reshape the images into 3D tensors so our CNN can use it
    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], X_train.shape[2], 1))
    X_val = np.reshape(X_val, (X_val.shape[0], X_val.shape[1], X_val.shape[2], 1))
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))

    return X_train, X_val, X_test

# Step 6: LOADING & PREPROCESSING DATA FUNCTION
def load_and_preprocess_data(debug=False):
    # Load the raw dataset
    (X_train, y_train), (X_test, y_test) = load_data()

    # Split test set into cross-validation and test sets
    X_val, y_val, X_test, y_test, test_images, test_labels = split_data(X_test, y_test)

    # Scale the images
    X_train, X_val, X_test = scale_images(X_train, X_val, X_test)

    # Reshape the images
    X_train, X_val, X_test = reshape_images(X_train, X_val, X_test)

    # Create TensorFlow datasets for training, validation, and test
    batch_size = 32  # Define your batch size
    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(buffer_size=len(X_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)
    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)

    if debug:
        print('After preprocessing:')
        print(' - X_train.shape = {}, y_train.shape = {}'.format(X_train.shape, y_train.shape))
        print(' - X_val.shape = {}, y_val.shape = {}'.format(X_val.shape, y_val.shape))
        print(' - X_test.shape = {}, y_test.shape = {}'.format(X_test.shape, y_test.shape))
        print(' - test_images.shape = {}, test_labels.shape = {}'.format(test_images.shape, test_labels.shape))

    return (train_dataset, val_dataset, test_dataset), (test_images, test_labels)

# Step 7: DEFINE DICTIONARY FOR FASHION LABELS
FASHION_LABELS = {
    0: 'T-shirt/top',
    1: 'Trouser',
    2: 'Pullover',
    3: 'Dress',
    4: 'Coat',
    5: 'Sandal',
    6: 'Shirt',
    7: 'Sneaker',
    8: 'Bag',
    9: 'Ankle boot',
}

# Step 8: SETUP SEABORN STYLING
sns.set_context("notebook", font_scale=1.1)
sns.set_style({"font.sans-serif": ["Verdana", "Arial", "Calibri", "DejaVu Sans"]})

# Step 9: DEFINE DISPLAY SAMPLE FUNCTION
def display_sample(sample_images, sample_labels, sample_predictions=None, num_rows=5, num_cols=10,
                   plot_title=None, fig_size=None):
    """ Display a random selection of images and corresponding labels """
    assert sample_images.shape[0] == num_rows * num_cols

    # Create subplots grid
    f, ax = plt.subplots(num_rows, num_cols, figsize=((14, 9) if fig_size is None else fig_size),
                         gridspec_kw={"wspace": 0.02, "hspace": 0.30}, squeeze=True)

    # Loop through rows and columns to plot the images and labels
    for r in range(num_rows):
        for c in range(num_cols):
            image_index = r * num_cols + c
            ax[r, c].axis("off")
            ax[r, c].imshow(sample_images[image_index], cmap="Greys")  # Show selected image

            # Handle predictions and actual labels
            if sample_predictions is None:
                title = ax[r, c].set_title("%s" % FASHION_LABELS[sample_labels[image_index]])
            else:
                true_label = sample_labels[image_index]
                pred_label = sample_predictions[image_index]
                prediction_matches_true = (sample_labels[image_index] == sample_predictions[image_index])

                if prediction_matches_true:
                    title = FASHION_LABELS[true_label]
                    title_color = 'g'
                else:
                    title = '%s/%s' % (FASHION_LABELS[true_label], FASHION_LABELS[pred_label])
                    title_color = 'r'

                plt.setp(title, color=title_color)

    # Set plot title, if one is specified
    if plot_title is not None:
        f.suptitle(plot_title)

    plt.show()
    plt.close()

# Step 10: LOAD DATA AND DISPLAY SAMPLES
(train_dataset, val_dataset, test_dataset), (test_images, test_labels) = load_and_preprocess_data(debug=True)

# Step 11: SAMPLE IMAGES AND DISPLAY THEM
sample_size = 30
rand_indexes = np.random.randint(0, len(test_images), sample_size)  # Use test_images length
sample_images = test_images[rand_indexes]
sample_labels = test_labels[rand_indexes]

# Adjusting number of rows and columns for display_sample
num_rows = 3
num_cols = 10

display_sample(sample_images, sample_labels,
               num_rows=num_rows, num_cols=num_cols,
               plot_title='Figure 1: Random sample of %d images' % sample_size)

"""#### BUILDING MODEL"""

# Step 1: Keras imports
import tensorflow.keras.backend as K
from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten,
                                     Dense, Dropout, BatchNormalization)
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

# Step 2: Define image dimensions and classes
Image_Height, Image_Width, Num_Channels, Num_Classes = 28, 28, 1, 10

# Step 3: Build the model
def build_model():
    """
    Builds a Keras CNN model for Fashion MNIST classification.
    """
    K.clear_session()  # Clear any previous sessions

    model = Sequential([
        Input(shape=(Image_Height, Image_Width, Num_Channels)),  # Add Input layer

        # First block of Conv2D, BatchNorm, and MaxPooling
        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        Conv2D(64, kernel_size=(3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),

        # Second block of Conv2D, BatchNorm, and MaxPooling
        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        Conv2D(128, kernel_size=(3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),

        # Third block of Conv2D, BatchNorm, and MaxPooling
        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),
        BatchNormalization(),
        Conv2D(256, kernel_size=(3, 3), activation='relu'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(2, 2)),

        # Flatten for the Dense layers
        Flatten(),

        # Fully connected Dense layers with Dropout for regularization
        Dense(1024, activation='relu'),
        Dropout(0.5),  # Add dropout to reduce overfitting
        Dense(512, activation='relu'),
        Dropout(0.5),  # Add dropout

        # Output layer
        Dense(Num_Classes, activation='softmax')  # Final layer with softmax for classification
    ])

    return model

"""#### MODEL TRAINING"""

# Step 1: Import the Adam Optimizer
from tensorflow.keras.optimizers import Adam

# Step 2: Ensure mixed precision is enabled
import tensorflow as tf
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Step 3: Define the train and evaluate model function
def train_and_evaluate_model(model, train_dataset, val_dataset, test_dataset,
                             plot_title=None,
                             callbacks=None, model_save_name=None,
                             num_epochs=25):
    """
    Cross-train & evaluate the model's performance on training, cross-validation & test datasets.
    """

    # Step 4: Train on train_dataset & cross-validate on val_dataset
    history = model.fit(train_dataset, epochs=num_epochs,
                        validation_data=val_dataset,
                        callbacks=callbacks)

    # Step 5: Display training history plots
    if plot_title is not None:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'], label='train_loss')
        plt.plot(history.history['val_loss'], label='val_loss')
        plt.title('Loss vs Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.legend()

        plt.subplot(1, 2, 2)
        plt.plot(history.history['accuracy'], label='train_accuracy')
        plt.plot(history.history['val_accuracy'], label='val_accuracy')
        plt.title('Accuracy vs Epochs')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.suptitle(plot_title)
        plt.show()

    # Step 6: Evaluate metrics
    print("\nEvaluating...", flush=True)

    print('Training data:', flush=True)
    loss, acc = model.evaluate(train_dataset, verbose=0)  # Set verbose to 0 for less output
    print("  Training : loss %.2f - acc %.2f" % (loss, acc))

    print('Cross-validation data:', flush=True)
    loss, acc = model.evaluate(val_dataset, verbose=0)
    print("  Cross-val: loss %.2f - acc %.2f" % (loss, acc))

    print('Test data:', flush=True)
    loss, acc = model.evaluate(test_dataset, verbose=0)
    print("  Testing  : loss %.2f - acc %.2f" % (loss, acc))

    if model_save_name is not None:
        print('Saving model...', flush=True)
        model.save(model_save_name)  # Save the model directly

# Step 7: Build and compile the model
model = build_model()  # Create the model
model.compile(optimizer=Adam(),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])  # Compile the model

# Step 8: Display the model architecture
model.summary()  # Show the model architecture

# Step 9: Define model name and plot title
model_name_base = 'kr_fashion_cnn_base.keras'  # Added the `.keras` extension
plot_title_base = 'Keras Base Model'

# Step 10: Train and evaluate the model without early stopping
train_and_evaluate_model(model, train_dataset, val_dataset, test_dataset,
                         plot_title=plot_title_base, model_save_name=model_name_base)  # Removed callbacks

# Step 11: Mount Google Drive to save the model persistently
from google.colab import drive
drive.mount('/content/drive')

# Step 12: Corrected save file path (saving to Google Drive)
model_name_base = '/content/drive/MyDrive/kr_fashion_cnn_base.keras'  # Use the `.keras` extension

# Step 13: Save the model
model.save(model_name_base)  # Save in Keras format

print(f"Model saved as {model_name_base}")

"""#### PREDICTIONS: PERFORMANCE EVALUATION ON TEST DATASET"""

import numpy as np
import tensorflow as tf
from google.colab import drive
import matplotlib.pyplot as plt

# Step 1: Mount Google Drive to save the model persistently
drive.mount('/content/drive')

# Step 2: Define the function to display sample predictions
def display_sample(sample_images, sample_labels, sample_predictions, num_rows, num_cols, plot_title=None):
    """ Display images with corresponding true and predicted labels """
    plt.figure(figsize=(num_cols * 2, num_rows * 2))
    for i in range(num_rows * num_cols):
        plt.subplot(num_rows, num_cols, i + 1)
        plt.imshow(sample_images[i].reshape(28, 28), cmap='gray')  # Ensure correct reshape for 28x28 images

        # Determine the color based on whether the prediction matches the true label
        if sample_labels[i] == sample_predictions[i]:
            color = 'green'  # Correct prediction
        else:
            color = 'red'    # Incorrect prediction

        plt.title(f"True: {FASHION_LABELS[sample_labels[i]]}\nPred: {FASHION_LABELS[sample_predictions[i]]}", fontsize=10, color=color)
        plt.axis('off')
    plt.subplots_adjust(hspace=0.5)  # Increase space between rows
    plt.suptitle(plot_title, fontsize=16)
    plt.show()

# Step 3: Define the function to do predictions
def do_predictions(model_save_name, X_test, y_test, test_images, test_labels, plot_title=None):
    def test_model(model, X_test, y_test, test_images, test_labels, plot_title=None):
        print('Evaluating against test data...', flush=True)

        # Get the total number of samples
        num_samples = X_test.shape[0]
        batch_size = 32  # Your defined batch size

        # Calculate total number of batches
        total_batches = np.ceil(num_samples / batch_size).astype(int)

        loss, acc = model.evaluate(X_test, y_test, verbose=0)
        print('  - Test data: loss %.2f - acc %.2f' % (loss, acc))

        print('Displaying sample predictions...', flush=True)
        # Run predictions
        y_pred = model.predict(X_test)

        # Test with 30 random images
        sample_size = 30
        rand_indexes = np.random.randint(0, len(X_test), sample_size)
        sample_images = test_images[rand_indexes]
        sample_labels = test_labels[rand_indexes]
        sample_predictions = np.argmax(y_pred[rand_indexes], axis=1)

        # Adjusting number of rows and columns for display_sample
        num_rows = 3
        num_cols = 10

        display_sample(sample_images, sample_labels, sample_predictions,
                       num_rows=num_rows, num_cols=num_cols,
                       plot_title=plot_title)

        # Print the number of batches processed
        print(f'{total_batches}/{total_batches} ━━━━━━━━━━━━━━━━━━━━')

    # Load the model from the saved state
    model = tf.keras.models.load_model(model_save_name)  # Load model from Google Drive
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # Use sparse_categorical_crossentropy for integer labels
    test_model(model, X_test, y_test, test_images, test_labels, plot_title=plot_title)
    del model  # Delete the model to free up memory

# Step 4: Load and preprocess data
def load_and_preprocess_data():
    # Load the raw dataset
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()

    # Simulate a split to create test images and labels
    test_images, test_labels = X_test, y_test  # In a real scenario, you may want to further split or preprocess

    # Scale the images
    X_test = X_test.astype('float32') / 255.0  # Scale to [0, 1]

    # Reshape the images into 3D tensors for CNN (28x28x1)
    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], X_test.shape[2], 1))

    return X_test, y_test, test_images, test_labels

# Step 5: Call the function to load data
X_test, y_test, test_images, test_labels = load_and_preprocess_data()

# Step 6: Call the function to do predictions
do_predictions('/content/drive/MyDrive/kr_fashion_cnn_base.keras', X_test, y_test, test_images, test_labels, plot_title="Figure 2: Random Sample Predictions Using The Test Dataset")